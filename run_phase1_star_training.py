#!/usr/bin/env python3
"""
PHASE 1 REDO: STAR Confidence Calibration with Real Diverse Datasets
Enhanced confidence calibration with real HumanEval/MBPP problems and confidence collapse prevention.

This script implements Phase 1 REDO of STAR (Strategic Training with Active Reinforcement):
- Use real diverse coding problems (HumanEval + MBPP + synthetic medium)
- Strategic selection of 200 diverse problems across difficulty levels
- Generate solutions with enhanced confidence scoring (prevents collapse)
- Train enhanced confidence calibrator with proper variance
- Validate calibration quality with ECE < 0.05 target and confidence diversity
"""

import logging
import sys
import os
import time
import signal
import psutil
import gc
from pathlib import Path
import json
import traceback
from datetime import datetime
from typing import Optional, List, Dict, Any
import numpy as np

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Import with bulletproof error handling
try:
    from sragv.orchestrator import SRAGVOrchestrator
    from sragv.models.solution_generator import SolutionGenerator
    from sragv.confidence_calibration import EnhancedConfidenceCalibrator
    from sragv.utils.apps_loader import APPSDataLoader
    from sragv.utils.simple_data_loader import SimpleAPPSDataLoader
    logger_import_success = True
except ImportError as e:
    print(f"CRITICAL IMPORT ERROR: {e}")
    logger_import_success = False

# Create comprehensive directory structure
for dir_name in ["logs", "checkpoints", "monitoring", "artifacts", "phase1_results"]:
    Path(dir_name).mkdir(exist_ok=True)

# Setup bulletproof logging with multiple outputs
log_filename = f"logs/phase1_star_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.DEBUG,  # More verbose for debugging
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_filename),
        logging.FileHandler('logs/phase1_star_training_latest.log')  # Always latest
    ]
)
logger = logging.getLogger(__name__)

# Global training state for graceful shutdown
training_state = {
    "orchestrator": None,
    "solution_generator": None,
    "should_stop": False,
    "current_phase": "initialization",
    "last_checkpoint": None,
    "calibration_data": []
}

def signal_handler(signum, frame):
    """Handle graceful shutdown on signals."""
    logger.info(f"üõë Received signal {signum}, initiating graceful shutdown...")
    training_state["should_stop"] = True
    
    if training_state["solution_generator"]:
        try:
            logger.info("üíæ Creating emergency checkpoint...")
            checkpoint_path = f"checkpoints/emergency_phase1_checkpoint_{int(time.time())}.pt"
            training_state["solution_generator"].save_calibrator(checkpoint_path)
            logger.info(f"Emergency checkpoint saved to {checkpoint_path}")
        except Exception as e:
            logger.error(f"‚ùå Emergency checkpoint failed: {e}")

# Register signal handlers
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def log_system_info():
    """Log comprehensive system information."""
    logger.info("üñ•Ô∏è PHASE 1 SYSTEM INFORMATION:")
    logger.info(f"  Python: {sys.version}")
    logger.info(f"  PID: {os.getpid()}")
    logger.info(f"  Working Directory: {os.getcwd()}")
    
    try:
        import torch
        logger.info(f"  PyTorch: {torch.__version__}")
        logger.info(f"  CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"  CUDA Version: {torch.version.cuda}")
            logger.info(f"  GPU Count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                logger.info(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
                
                # Check GPU memory usage
                memory_allocated = torch.cuda.memory_allocated(i) / 1e9
                memory_cached = torch.cuda.memory_reserved(i) / 1e9
                logger.info(f"    Memory: {memory_allocated:.1f}GB allocated, {memory_cached:.1f}GB cached")
        
        # Enhanced GPU functionality and ECC test
        if torch.cuda.is_available():
            # Check for ECC errors before starting
            import subprocess
            try:
                ecc_result = subprocess.run(['nvidia-smi', '--query-gpu=ecc.errors.corrected.total,ecc.errors.uncorrected.total', '--format=csv,noheader,nounits'], 
                                          capture_output=True, text=True, timeout=10)
                ecc_lines = ecc_result.stdout.strip().split('\n')
                for i, line in enumerate(ecc_lines):
                    if line and ',' in line:
                        corrected, uncorrected = line.split(',')
                        corrected, uncorrected = corrected.strip(), uncorrected.strip()
                        if corrected != '0' or uncorrected != '0':
                            logger.warning(f"‚ö†Ô∏è GPU {i} has ECC errors: Corrected={corrected}, Uncorrected={uncorrected}")
                            logger.warning("This may cause training instability!")
                        else:
                            logger.info(f"  GPU {i} ECC: ‚úÖ Clean (no errors)")
            except Exception as e:
                logger.warning(f"Could not check ECC status: {e}")
            
            # Multi-round GPU stress test
            for test_round in range(3):
                try:
                    test_tensor = torch.randn(2000, 2000, device='cuda:0')
                    result = torch.matmul(test_tensor, test_tensor.T).sum()
                    torch.cuda.synchronize()
                    logger.info(f"  GPU Stress Test {test_round + 1}/3: ‚úÖ PASSED (result: {result.item():.2e})")
                    del test_tensor, result
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.error(f"  GPU Stress Test {test_round + 1}/3: ‚ùå FAILED ({e})")
                    logger.error("GPU may be unstable - this could cause training failures!")
                    raise RuntimeError(f"GPU stress test failed: {e}")
            
    except Exception as e:
        logger.error(f"‚ùå Failed to get PyTorch info: {e}")
    
    # System resources
    try:
        memory = psutil.virtual_memory()
        logger.info(f"  RAM: {memory.total / 1e9:.1f}GB total, {memory.available / 1e9:.1f}GB available")
        logger.info(f"  CPU: {psutil.cpu_count()} cores, {psutil.cpu_percent(interval=1):.1f}% usage")
        
        # Disk space
        disk = psutil.disk_usage('/')
        logger.info(f"  Disk: {disk.total / 1e9:.1f}GB total, {disk.free / 1e9:.1f}GB free")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to get system info: {e}")

def validate_prerequisites() -> bool:
    """Validate all prerequisites for Phase 1 training."""
    logger.info("üîç Validating Phase 1 prerequisites...")
    
    checks = []
    
    # Check required files exist
    required_files = [
        "config/config.yaml",
        "src/sragv/models/solution_generator.py",
        "src/sragv/confidence_calibration.py"
    ]
    
    for file_path in required_files:
        exists = Path(file_path).exists()
        checks.append((f"Required File: {file_path}", exists))
        if not exists:
            logger.error(f"‚ùå Missing required file: {file_path}")
    
    # Check GPU availability for training
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        checks.append(("GPU Availability", gpu_available))
        
        if gpu_available:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            sufficient_memory = gpu_memory >= 8.0  # Need at least 8GB for 7B model
            checks.append((f"GPU Memory >= 8GB ({gpu_memory:.1f}GB)", sufficient_memory))
        else:
            logger.error("‚ùå GPU not available - Phase 1 requires GPU for training")
            
    except Exception as e:
        logger.error(f"‚ùå GPU check failed: {e}")
        checks.append(("GPU Check", False))
    
    # Check disk space (need ~10GB for training data and checkpoints)
    try:
        disk = psutil.disk_usage('/')
        sufficient_disk = disk.free / 1e9 >= 10.0
        checks.append((f"Disk Space >= 10GB ({disk.free / 1e9:.1f}GB)", sufficient_disk))
    except Exception as e:
        logger.error(f"‚ùå Disk space check failed: {e}")
        checks.append(("Disk Space Check", False))
    
    # Report results
    all_passed = True
    logger.info("üìã Phase 1 Prerequisites Check:")
    for check_name, passed in checks:
        status = "‚úÖ" if passed else "‚ùå"
        logger.info(f"  {status} {check_name}")
        if not passed:
            all_passed = False
    
    if not all_passed:
        logger.error("‚ùå Prerequisites failed. Cannot proceed with Phase 1 training.")
        return False
    
    logger.info("‚úÖ All Phase 1 prerequisites validated successfully")
    return True

def create_phase1_config() -> Dict[str, Any]:
    """Create optimized configuration for Phase 1 STAR training."""
    logger.info("‚öôÔ∏è Creating Phase 1 STAR training configuration...")
    
    config = {
        # Data configuration
        "apps_problems_count": 2000,  # Total problems for calibration
        "solutions_per_problem": 6,   # Generate 6 solutions per problem
        "train_split": 0.8,          # 80% for training, 20% for validation
        
        # Model configuration
        "batch_size": 32,            # Batch size for solution generation
        "max_solutions_parallel": 8, # Process solutions in parallel
        
        # Calibration training configuration
        "calibration_epochs": 50,
        "calibration_lr": 0.01,
        "early_stopping_patience": 10,
        
        # Validation configuration
        "target_ece": 0.1,           # Target Expected Calibration Error < 0.1
        "validation_frequency": 5,   # Validate every 5 epochs
        
        # Resource management (conservative for flaky GPUs)
        "gpu_memory_fraction": 0.7,  # Use only 70% of GPU memory (safer)
        "checkpoint_frequency": 5,   # More frequent checkpoints
        "memory_cleanup_frequency": 4,  # Clean memory every 4 batches
        
        # Quality gates
        "min_syntax_valid_rate": 0.7,   # At least 70% syntactically valid solutions
        "max_calibration_ece": 0.15,    # Fail if ECE > 0.15
        "min_improvement_threshold": 0.001  # Minimum improvement to continue training
    }
    
    logger.info(f"Phase 1 Configuration:")
    logger.info(f"  - {config['apps_problems_count']} APPS problems")
    logger.info(f"  - {config['solutions_per_problem']} solutions per problem")
    logger.info(f"  - Target ECE: {config['target_ece']}")
    logger.info(f"  - Calibration epochs: {config['calibration_epochs']}")
    
    return config

def prepare_apps_dataset(config: Dict[str, Any]) -> tuple:
    """Prepare diverse real dataset for Phase 1 REDO training."""
    logger.info(f"üìä PHASE 1 REDO: Preparing diverse real coding problems...")
    training_state["current_phase"] = "diverse_data_preparation"
    
    # Step 1: Download real datasets
    logger.info("üåê Downloading real coding datasets...")
    try:
        import subprocess
        result = subprocess.run([
            sys.executable, "download_real_datasets.py"
        ], capture_output=True, text=True, timeout=600)  # 10 min timeout
        
        if result.returncode != 0:
            logger.warning(f"Dataset download had issues: {result.stderr}")
        else:
            logger.info("‚úÖ Real datasets downloaded successfully")
    except Exception as e:
        logger.warning(f"Could not run dataset download: {e}")
    
    # Step 2: Strategic selection of diverse problems
    logger.info("üéØ Strategic selection of diverse problems...")
    try:
        import subprocess
        result = subprocess.run([
            sys.executable, "select_diverse_phase1_problems.py"
        ], capture_output=True, text=True, timeout=300)  # 5 min timeout
        
        if result.returncode != 0:
            logger.warning(f"Problem selection had issues: {result.stderr}")
        else:
            logger.info("‚úÖ Diverse problems selected successfully")
    except Exception as e:
        logger.warning(f"Could not run problem selection: {e}")
    
    # Step 3: Load selected diverse problems
    diverse_problems_path = Path("data/phase1_diverse_problems.json")
    if diverse_problems_path.exists():
        with open(diverse_problems_path, 'r') as f:
            problems = json.load(f)
        logger.info(f"‚úÖ Loaded {len(problems)} strategically selected diverse problems")
        
        # Analyze diversity
        difficulties = {}
        sources = {}
        for prob in problems:
            diff = prob.get('difficulty', 'unknown')
            source = prob.get('source', 'unknown')
            difficulties[diff] = difficulties.get(diff, 0) + 1
            sources[source] = sources.get(source, 0) + 1
        
        logger.info(f"üìä Difficulty distribution: {difficulties}")
        logger.info(f"üìä Source distribution: {sources}")
        
        # Create categorized dict for compatibility
        categorized = {'diverse_real': problems}
        
    else:
        logger.warning("‚ö†Ô∏è Diverse problems file not found, falling back to synthetic")
        # Fallback to synthetic problem generation
        simple_loader = SimpleAPPSDataLoader(
            cache_dir="./data",
            max_problems=min(200, config['apps_problems_count'])
        )
        problems, categorized = simple_loader.prepare_bootstrap_data()
        logger.info(f"‚úÖ Generated {len(problems)} synthetic fallback problems")
    
    # Split into train/validation (80/20 split)
    split_idx = int(len(problems) * config['train_split'])
    train_problems = problems[:split_idx]
    val_problems = problems[split_idx:]
    
    logger.info(f"üìà PHASE 1 REDO Dataset: {len(train_problems)} training, {len(val_problems)} validation")
    logger.info(f"üéØ Enhanced confidence scoring enabled to prevent collapse")
    
    # Save dataset for reproducibility
    dataset_info = {
        'phase': '1_REDO',
        'total_problems': len(problems),
        'train_problems': len(train_problems),
        'val_problems': len(val_problems),
        'categorized_counts': {k: len(v) for k, v in categorized.items()} if categorized else {},
        'confidence_collapse_prevention': 'enabled',
        'timestamp': datetime.now().isoformat()
    }
    
    with open('phase1_results/phase1_redo_dataset_info.json', 'w') as f:
        json.dump(dataset_info, f, indent=2)
    
    logger.info("‚úÖ PHASE 1 REDO dataset preparation complete")
    return train_problems, val_problems

def generate_calibration_solutions(
    solution_generator: SolutionGenerator,
    problems: List[Dict],
    config: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """Generate solutions for calibration training."""
    logger.info(f"üîß Generating solutions for {len(problems)} problems...")
    training_state["current_phase"] = "solution_generation"
    
    all_solution_data = []
    batch_size = config['batch_size']
    solutions_per_problem = config['solutions_per_problem']
    
    # Process problems in batches
    for i in range(0, len(problems), batch_size):
        if training_state["should_stop"]:
            logger.info("üõë Stopping solution generation due to shutdown signal")
            break
            
        batch_problems = problems[i:i + batch_size]
        logger.info(f"Processing batch {i//batch_size + 1}/{(len(problems) + batch_size - 1)//batch_size}")
        
        batch_start_time = time.time()
        
        for problem in batch_problems:
            try:
                # Generate solutions for this problem
                solutions = solution_generator.generate(
                    problem=problem,
                    num_solutions=solutions_per_problem,
                    max_attempts_per_solution=2  # Limit attempts for speed
                )
                
                # Create calibration training data
                for solution in solutions:
                    # Compute ground truth score (using execution results as proxy)
                    execution_results = solution.get('validation_results', [])
                    if execution_results:
                        pass_rate = solution.get('pass_rate', 0.0)
                        true_score = pass_rate  # Use pass rate as ground truth
                    else:
                        # Fall back to heuristic if no execution results
                        true_score = solution.get('score', 0.0)
                    
                    # Get base heuristic score for comparison
                    base_score = solution_generator._compute_heuristic_score(
                        solution.get('code', ''), problem
                    )
                    
                    solution_data = {
                        'problem_id': problem.get('problem_id', 'unknown'),
                        'code': solution.get('code', ''),
                        'problem': problem,
                        'base_score': base_score,
                        'true_score': true_score,
                        'generation_temperature': solution.get('temperature', 0.5),
                        'solution_length': len(solution.get('code', '').split('\n')),
                        'syntactically_valid': solution.get('syntax_valid', False)
                    }
                    
                    all_solution_data.append(solution_data)
                
            except Exception as e:
                logger.error(f"‚ùå Error generating solutions for problem {problem.get('problem_id', 'unknown')}: {e}")
                continue
        
        batch_time = time.time() - batch_start_time
        logger.info(f"Batch completed in {batch_time:.1f}s, {len(all_solution_data)} total solutions")
        
        # Memory cleanup
        if i % (batch_size * 4) == 0:  # Every 4 batches
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    # Filter out invalid solutions
    valid_solutions = [s for s in all_solution_data if len(s['code']) > 10]
    syntax_valid_rate = sum(1 for s in valid_solutions if s['syntactically_valid']) / len(valid_solutions) if valid_solutions else 0
    
    logger.info(f"‚úÖ Solution generation complete:")
    logger.info(f"  - Total solutions: {len(valid_solutions)}")
    logger.info(f"  - Syntax valid rate: {syntax_valid_rate:.2%}")
    logger.info(f"  - Average solution length: {np.mean([s['solution_length'] for s in valid_solutions]):.1f} lines")
    
    # Quality gate check
    if syntax_valid_rate < config['min_syntax_valid_rate']:
        logger.warning(f"‚ö†Ô∏è Syntax valid rate {syntax_valid_rate:.2%} below threshold {config['min_syntax_valid_rate']:.2%}")
    
    # Save solution data
    with open('phase1_results/solution_data.json', 'w') as f:
        json.dump(valid_solutions, f, indent=2, default=str)
    
    return valid_solutions

def train_confidence_calibrator(
    solution_generator: SolutionGenerator,
    train_data: List[Dict],
    val_data: List[Dict],
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """Train the confidence calibrator with enhanced monitoring."""
    logger.info(f"üß† Training confidence calibrator on {len(train_data)} samples...")
    training_state["current_phase"] = "calibration_training"
    
    # Train the calibrator
    training_metrics = solution_generator.train_confidence_calibrator(
        training_data=train_data,
        validation_data=val_data,
        num_epochs=config['calibration_epochs'],
        learning_rate=config['calibration_lr']
    )
    
    logger.info("‚úÖ Calibration training complete")
    return training_metrics

def validate_calibration_quality(
    solution_generator: SolutionGenerator,
    val_data: List[Dict],
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """Validate calibration quality with comprehensive metrics."""
    logger.info(f"üéØ Validating calibration quality on {len(val_data)} samples...")
    training_state["current_phase"] = "calibration_validation"
    
    # Evaluate calibration metrics
    calibration_metrics = solution_generator.evaluate_calibration_quality(val_data)
    
    # Check quality gates
    ece = calibration_metrics.get('ece', 1.0)
    mce = calibration_metrics.get('mce', 1.0)
    brier_score = calibration_metrics.get('brier_score', 1.0)
    
    logger.info("üéØ Calibration Quality Results:")
    logger.info(f"  - Expected Calibration Error (ECE): {ece:.4f}")
    logger.info(f"  - Maximum Calibration Error (MCE): {mce:.4f}")
    logger.info(f"  - Brier Score: {brier_score:.4f}")
    
    # Quality assessment
    if ece <= config['target_ece']:
        logger.info("üéâ ‚úÖ CALIBRATION TARGET ACHIEVED!")
        logger.info(f"    ECE {ece:.4f} ‚â§ target {config['target_ece']}")
        quality_status = "excellent"
    elif ece <= config['max_calibration_ece']:
        logger.info("‚úÖ Calibration quality acceptable")
        quality_status = "acceptable"
    else:
        logger.warning(f"‚ö†Ô∏è Calibration quality below threshold (ECE: {ece:.4f})")
        quality_status = "poor"
    
    calibration_metrics['quality_status'] = quality_status
    calibration_metrics['target_achieved'] = ece <= config['target_ece']
    
    return calibration_metrics

def run_phase1_training(config: Dict[str, Any]) -> Dict[str, Any]:
    """Run complete Phase 1 STAR training with monitoring."""
    logger.info("üöÄ Starting Phase 1 STAR training execution...")
    
    start_time = time.time()
    results = {
        "phase": "phase1_star_calibration",
        "status": "running",
        "start_time": datetime.now().isoformat(),
        "stages": {}
    }
    
    try:
        # Stage 1: Initialize system
        logger.info("üîß Stage 1: System Initialization")
        stage_start = time.time()
        
        orchestrator = SRAGVOrchestrator("config/config.yaml")
        orchestrator.initialize_players()
        
        solution_generator = orchestrator.solution_generator
        training_state["orchestrator"] = orchestrator
        training_state["solution_generator"] = solution_generator
        
        results["stages"]["initialization"] = {
            "status": "completed",
            "duration": time.time() - stage_start
        }
        logger.info(f"‚úÖ Stage 1 complete ({results['stages']['initialization']['duration']:.1f}s)")
        
        # Stage 2: Prepare dataset
        logger.info("üìä Stage 2: Dataset Preparation")
        stage_start = time.time()
        
        train_problems, val_problems = prepare_apps_dataset(config)
        
        results["stages"]["dataset_preparation"] = {
            "status": "completed",
            "duration": time.time() - stage_start,
            "train_problems": len(train_problems),
            "val_problems": len(val_problems)
        }
        logger.info(f"‚úÖ Stage 2 complete ({results['stages']['dataset_preparation']['duration']:.1f}s)")
        
        # Stage 3: Generate solutions
        logger.info("üîß Stage 3: Solution Generation")
        stage_start = time.time()
        
        train_solutions = generate_calibration_solutions(solution_generator, train_problems, config)
        val_solutions = generate_calibration_solutions(solution_generator, val_problems, config)
        
        results["stages"]["solution_generation"] = {
            "status": "completed", 
            "duration": time.time() - stage_start,
            "train_solutions": len(train_solutions),
            "val_solutions": len(val_solutions)
        }
        logger.info(f"‚úÖ Stage 3 complete ({results['stages']['solution_generation']['duration']:.1f}s)")
        
        # Stage 4: Train calibrator
        logger.info("üß† Stage 4: Calibration Training")
        stage_start = time.time()
        
        training_metrics = train_confidence_calibrator(
            solution_generator, train_solutions, val_solutions, config
        )
        
        results["stages"]["calibration_training"] = {
            "status": "completed",
            "duration": time.time() - stage_start,
            "training_metrics": training_metrics
        }
        logger.info(f"‚úÖ Stage 4 complete ({results['stages']['calibration_training']['duration']:.1f}s)")
        
        # Stage 5: Validate calibration
        logger.info("üéØ Stage 5: Calibration Validation")
        stage_start = time.time()
        
        calibration_metrics = validate_calibration_quality(solution_generator, val_solutions, config)
        
        results["stages"]["calibration_validation"] = {
            "status": "completed",
            "duration": time.time() - stage_start,
            "calibration_metrics": calibration_metrics
        }
        logger.info(f"‚úÖ Stage 5 complete ({results['stages']['calibration_validation']['duration']:.1f}s)")
        
        # Final results
        total_time = time.time() - start_time
        results.update({
            "status": "completed",
            "total_duration": total_time,
            "end_time": datetime.now().isoformat(),
            "final_ece": calibration_metrics.get('ece', 1.0),
            "target_achieved": calibration_metrics.get('target_achieved', False),
            "quality_status": calibration_metrics.get('quality_status', 'unknown')
        })
        
        # Save calibrator
        calibrator_path = f"checkpoints/phase1_star_calibrator_{int(time.time())}.pt"
        solution_generator.save_calibrator(calibrator_path)
        results["calibrator_path"] = calibrator_path
        
        logger.info("üéâ Phase 1 STAR training completed successfully!")
        logger.info(f"Total time: {total_time:.1f}s ({total_time/60:.1f}min)")
        logger.info(f"Final ECE: {results['final_ece']:.4f}")
        logger.info(f"Target achieved: {results['target_achieved']}")
        
        return results
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Phase 1 training interrupted by user")
        results.update({
            "status": "interrupted",
            "total_duration": time.time() - start_time,
            "error": "User interrupted"
        })
        return results
        
    except Exception as e:
        logger.error(f"üí• Phase 1 training failed: {e}")
        logger.error("Full traceback:")
        for line in traceback.format_exc().splitlines():
            logger.error(f"  {line}")
        
        results.update({
            "status": "failed",
            "total_duration": time.time() - start_time,
            "error": str(e),
            "traceback": traceback.format_exc()
        })
        return results

def analyze_and_report_results(results: Dict[str, Any]):
    """Analyze and report comprehensive Phase 1 results."""
    logger.info("üìä PHASE 1 STAR TRAINING RESULTS ANALYSIS")
    logger.info("=" * 80)
    
    status = results.get("status", "unknown")
    total_time = results.get("total_duration", 0)
    
    if status == "completed":
        logger.info("üéâ ‚úÖ PHASE 1 TRAINING COMPLETED SUCCESSFULLY!")
        logger.info(f"‚è±Ô∏è Total training time: {total_time:.1f}s ({total_time/60:.1f}min)")
        
        # Analyze calibration quality
        final_ece = results.get("final_ece", 1.0)
        target_achieved = results.get("target_achieved", False)
        quality_status = results.get("quality_status", "unknown")
        
        if target_achieved:
            logger.info("üèÜ BREAKTHROUGH ACHIEVED!")
            logger.info(f"   ‚úì ECE target achieved: {final_ece:.4f} ‚â§ 0.1")
            logger.info("   ‚úì STAR calibration ready for Phase 2")
            logger.info("   ‚úì Enhanced confidence scoring operational")
        elif quality_status == "acceptable":
            logger.info("üìà STRONG BASELINE ESTABLISHED!")
            logger.info(f"   ‚úì Acceptable calibration quality: ECE = {final_ece:.4f}")
            logger.info("   ‚úì STAR system functional, ready for optimization")
        else:
            logger.info("‚ö†Ô∏è TRAINING COMPLETED WITH ROOM FOR IMPROVEMENT")
            logger.info(f"   - ECE: {final_ece:.4f} (target: ‚â§ 0.1)")
            logger.info("   - Consider additional training or parameter tuning")
        
        # Stage breakdown
        stages = results.get("stages", {})
        for stage_name, stage_info in stages.items():
            duration = stage_info.get("duration", 0)
            logger.info(f"   {stage_name}: {duration:.1f}s")
    
    elif status == "interrupted":
        logger.info("‚èπÔ∏è Phase 1 training was interrupted")
        logger.info("   Partial results may be available in checkpoints")
        
    elif status == "failed":
        error = results.get("error", "Unknown error")
        logger.info(f"üí• Phase 1 training failed: {error}")
        logger.info("   Check logs for detailed error information")
    
    logger.info("=" * 80)
    
    # Save detailed report
    report_path = Path("phase1_results/phase1_final_report.json")
    with open(report_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    logger.info(f"üìã Detailed Phase 1 report saved to: {report_path}")

def cleanup_resources():
    """Clean up resources and memory."""
    logger.info("üßπ Cleaning up Phase 1 resources...")
    try:
        # Force garbage collection
        gc.collect()
        
        # Clear CUDA cache if available
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("  ‚úÖ CUDA cache cleared")
        except:
            pass
        
        # Clear training state
        training_state["orchestrator"] = None
        training_state["solution_generator"] = None
        training_state["should_stop"] = False
        training_state["calibration_data"] = []
        
        logger.info("  ‚úÖ Phase 1 resource cleanup complete")
    except Exception as e:
        logger.error(f"  ‚ùå Cleanup error: {e}")

def main():
    """BULLETPROOF main execution function for Phase 1 STAR training."""
    start_time = time.time()
    exit_code = 1  # Default to failure
    
    try:
        logger.info("üõ°Ô∏è BULLETPROOF SRAG-V PHASE 1: STAR CONFIDENCE CALIBRATION")
        logger.info("=" * 80)
        logger.info("Implementing breakthrough-level confidence calibration with:")
        logger.info("  ‚úì Enhanced confidence calibrator with temperature scaling")
        logger.info("  ‚úì 2000 APPS problems with ground truth labels")
        logger.info("  ‚úì 6 solutions per problem for robust calibration")
        logger.info("  ‚úì Expected Calibration Error (ECE) < 0.1 target")
        logger.info("  ‚úì Reliability diagrams and comprehensive validation")
        logger.info("  ‚úì Bulletproof error handling & recovery")
        logger.info("  ‚úì Comprehensive monitoring & logging")
        logger.info("=" * 80)
        
        # Log system information
        log_system_info()
        
        # Check import success
        if not logger_import_success:
            logger.error("‚ùå Critical imports failed - cannot proceed")
            with open('/workspace/training_status.txt', 'w') as f:
                f.write('FAILED:IMPORT_ERROR')
            return 1
        
        # Step 1: Validate prerequisites
        logger.info("üîç Step 1: Prerequisites Validation")
        try:
            if not validate_prerequisites():
                logger.error("‚ùå Prerequisites validation failed")
                with open('/workspace/training_status.txt', 'w') as f:
                    f.write('FAILED:PREREQUISITES')
                return 1
            logger.info("‚úÖ Prerequisites validated successfully")
        except Exception as e:
            logger.error(f"‚ùå Prerequisites validation error: {e}")
            with open('/workspace/training_status.txt', 'w') as f:
                f.write(f'FAILED:PREREQUISITES_ERROR:{str(e)[:100]}')
            return 1
        
        # Step 2: Create Phase 1 configuration
        logger.info("‚öôÔ∏è Step 2: Configuration Creation")
        try:
            config = create_phase1_config()
            logger.info("‚úÖ Phase 1 configuration created successfully")
        except Exception as e:
            logger.error(f"‚ùå Configuration creation failed: {e}")
            with open('/workspace/training_status.txt', 'w') as f:
                f.write(f'FAILED:CONFIG_ERROR:{str(e)[:100]}')
            return 1
        
        # Step 3: Run Phase 1 training
        logger.info("üöÄ Step 3: Phase 1 Training Execution")
        try:
            results = run_phase1_training(config)
            logger.info("‚úÖ Phase 1 training execution completed")
        except Exception as e:
            logger.error(f"‚ùå Phase 1 training failed: {e}")
            with open('/workspace/training_status.txt', 'w') as f:
                f.write(f'FAILED:TRAINING_ERROR:{str(e)[:100]}')
            return 1
        
        # Step 4: Analyze results
        logger.info("üìä Step 4: Results Analysis")
        try:
            analyze_and_report_results(results)
            logger.info("‚úÖ Results analysis completed")
        except Exception as e:
            logger.error(f"‚ùå Results analysis failed: {e}")
            # Continue since training is done
        
        # Step 5: Write success status
        final_status = results.get("status", "unknown")
        target_achieved = results.get("target_achieved", False)
        
        if final_status == "completed" and target_achieved:
            with open('/workspace/training_status.txt', 'w') as f:
                f.write('SUCCESS:TARGET_ACHIEVED')
            exit_code = 0
            logger.info("üéâ Phase 1 STAR training SUCCESS - ECE target achieved!")
            
        elif final_status == "completed":
            with open('/workspace/training_status.txt', 'w') as f:
                f.write('SUCCESS:BASELINE_ESTABLISHED')
            exit_code = 0
            logger.info("‚úÖ Phase 1 STAR training SUCCESS - baseline established!")
            
        else:
            with open('/workspace/training_status.txt', 'w') as f:
                f.write(f'FAILED:{final_status.upper()}')
            logger.error(f"‚ùå Phase 1 training ended with status: {final_status}")
        
        return exit_code
        
    except KeyboardInterrupt:
        logger.info("üõë Phase 1 training interrupted by user")
        with open('/workspace/training_status.txt', 'w') as f:
            f.write('INTERRUPTED:USER')
        return 1
        
    except Exception as e:
        logger.error(f"üí• Critical Phase 1 error: {e}")
        logger.error("Full traceback:")
        for line in traceback.format_exc().splitlines():
            logger.error(f"  {line}")
        
        with open('/workspace/training_status.txt', 'w') as f:
            f.write(f'FAILED:CRITICAL_ERROR:{str(e)[:100]}')
        return 1
        
    finally:
        # Always cleanup
        cleanup_resources()
        
        # Log final timing
        total_time = time.time() - start_time
        logger.info(f"üèÅ Phase 1 total execution time: {total_time:.1f}s ({total_time/60:.1f}min)")

if __name__ == "__main__":
    sys.exit(main())